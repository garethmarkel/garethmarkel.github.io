---
title: 'PRScs.jl: Bayesian Regression for GWAS summary statistics'
date: 2024-12-31
permalink: /posts/prs-cs-jl/
---
This post is about a Bayesian regression tool I implemented for fitting machine learning predictors on genetic data. The code for this can be found at this [GitHub link](https://github.com/garethmarkel/PRScs.jl).

#### Technologies and topics used: Julia, Python, Bayesian statistics, MCMC, Machine learning, Quantitative genetics, Regression

A central application of genetic data is understanding and predicting disease risk.  Some diseases are monogenic--meaning we can use the presence of a single gene as a decision rule--but the human genome (the code that drives our biology) is over 3 billion base pairs, so many diseases (and other traits) are driven by not one but many genes--we call these traits "polygenic."

When we study polygenic diseases, we usually use data on single nucleotide polymorphisms (SNPs). This is loosely defined as a variation at a single base pair that occurs in at least 1% of the population. Individuals in the [1000 Genomes Project](https://en.wikipedia.org/wiki/1000_Genomes_Project) had over 84 million SNPs between about 2500 people. So even as you built large datasets of genotyped individuals, you run into "big P little N" problems, at least in the application of classical statistics--you obviously can't plug all those SNPs into a regression. 

This is what brings us to the "polygenic risk score." It's a machine learning predictor of genetic risk for a disease, or propensity for a non-disease trait, developed using methodology tailored to the unique problems of genomic research. Basically, the way it works is:

1. Regress a phenotype $$y$$ on $$X_j$$ the standardized number of a "reference allele" (one of A,C,G,T) at a given base pair $$j$$, for every base pair (and some controls). This number of reference alleles is ideally {0,1,2}, but may be an imputed value on [0,2]. 
2. Store the resulting estimates $$\hat{\beta} = X_j^TY/N$$ (these estimates are obviously contaminated by omitted variable bias--each regression is picking up signal from many neighboring SNPs).
3. Sum the fitted values from each base pair regression--for person $$i$$, this gives $$PRS_i = \Sigma_j\hat{\beta_j}x_{j,i}$$

Some obvious flaws jump out from this. Each individual OLS is going to pick up a ton of extra signal, since proximal genotypes are correlated. We can somewhat correct this through the use of an LD panel, which tracks the correlation of nearby SNPs, but if your panel population doesn't match your GWAS population this can introduce bias. If your sample is too small (and a million observations may be "too small" for some effects), each regression is also going to pick up a ton of noise. Finally, just summing the fitted values is very crude. 

Luckily, the VAST majority of true SNP associations are very, very close to zero. That, and the regression setup of GWAS, means we can apply machine learning  techniques to enhance the weights for the PRS. Essentially, it's a high dimensional sparse regression problem, which is a subset of Bayesian machine learning very important for genetic and imaging applications. Basically, we just take the estimates in step 2 and regularize them. There are a few methods to do this, but here I'm going to talk about continuous shrinkage priors.

Continuous shrinkage priors adapt the level of shrinkage for each SNP, to the probable strength of signal for that SNP, so they're flexible to different genetic architectures. They also have some tricks that make them very computationally tractable, which made them very attractive in the 2010s and 2000s. The idea is that we model the distribution of effect sizes as concentrated at 0, with heavy tails to allow for non-zero signals (see the below image). This means the software will shrink estimates that are just noise, but leave probable true signals alone.

 This can usually be modelled as a mixture of normals, which makes implementing a Gibbs sampler straightforward. [Ge et al](https://github.com/getian107/PRScs/tree/master) explored this with their software PRS-CS, but only implemented priors that can be expressed through the [Strawderman-Berger](https://ssca.org.in/media/22_Vol._18_No._2_2020_SA_Anindya__Roy.pdf) formulation. As a personal interest project, I reimplemented this in Julia, adding support for an additional prior, the [Dirichlet Laplace](https://arxiv.org/pdf/1602.01160). The code for this can be found at this [GitHub link](https://github.com/garethmarkel/PRScs.jl).



![image](images/dl_prior)



> Written with [StackEdit](https://stackedit.io/).